"use strict";(self.webpackChunkposts=self.webpackChunkposts||[]).push([[849],{8580:e=>{e.exports=JSON.parse("{\"blogPosts\":[{\"id\":\"teaching-an-llm-how-to-read\",\"metadata\":{\"permalink\":\"/posts/teaching-an-llm-how-to-read\",\"source\":\"@site/blog/2023-11-10-teaching-an-llm-how-to-read/index.md\",\"title\":\"Teaching an LLM How to Read\",\"description\":\"cover\",\"date\":\"2023-11-10T00:00:00.000Z\",\"formattedDate\":\"November 10, 2023\",\"tags\":[],\"readingTime\":7.87,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"teaching-an-llm-how-to-read\",\"title\":\"Teaching an LLM How to Read\"},\"unlisted\":false,\"nextItem\":{\"title\":\"Generating Narratives: Writing Coherent Stories with LLMs\",\"permalink\":\"/posts/generating-narratives\"}},\"content\":\"![cover](./cover.png)\\n\\n[Retrieval Augmented Generation (RAG)](https://arxiv.org/pdf/2205.14704.pdf) has become a standard approach to building LLM interfaces on top of specific data, but it can struggle in cases where the input doesn't overlap meaningfully with the stored embeddings.\\n\\nIn this post, I want to explore a solution to this by introducing a ***thought-generation*** step. The process works as follows:\\n\\n1. Text chunks are first passed to a prompt to generate thoughts about the passage.\\n2. The generated thoughts are then embedded\\n3. RAG interface uses the generated thoughts as context\\n\\nAll tests are done using `chatgpt-3.5-turbo` and can be run using the [notebook](https://github.com/layterz/promptx/blob/main/examples/arxiv-reader/arxiv-reader.ipynb).\\n\\n\x3c!--truncate--\x3e\\n\\n## Setting up promptx\\n\\nWe're going to use [promptx](https://github.com/layterz/promptx) to handle the LLM interaction and embeddings.\\n\\n```bash\\npip install pxx\\n```\\n\\nAnd now we need to configure the LLM we want to use.\\n\\n```bash\\nexport PXX_DEFAULT_LLM=chatgpt\\nexport PXX_OPENAI_API_KEY=...\\nexport PXX_OPENAI_ORG_ID=...\\n```\\n\\nTo see how to use other models take a look at the [documentation](https://github.com/layterz/promptx#configuration).\\n\\nNow, let's test that it's working.\\n\\n```python\\nfrom promptx import prompt\\n\\noutput = prompt('Where is the capital of France?')\\nassert 'Paris' in output\\n```\\n\\n## Storing quotes\\n\\nI'm using the paper [DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase](https://arxiv.org/abs/2311.03319). It uses an ensemble approach to boost the performance of prompting generative models.\\n\\nLet's start by simply storing each sentence from the paper as a quote. When you store something in **promptx** it creates an embedding, which will allow us to query the quotes to augment question answering and other tasks.\\n\\nFirst, we need to define `Quote` and `Document` entities. The `Entity` class is a thin wrapper on top of `pydantic.BaseModel`, which allows it to be stored as an embedding.\\n\\n```python\\nfrom promptx import Entity\\n\\nclass Document(Entity):\\n    title: str\\n    authors: list[str]\\n    abstract: str\\n    url: str\\n\\nclass Quote(Entity):\\n    text: str\\n    source_id: str\\n    start: int\\n    end: int\\n```\\n\\nNow, we need to iterate over the paper and create the quote embeddings. I'm using `spacy` to split the paper into sentences, but you can use `nltk` or whatever method you want as long as the text is split into small enough chunks to be meaningfully embedded.\\n\\n```python\\nimport spacy\\nfrom promptx import store\\n\\npaper = 'DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase...'\\nnlp = spacy.load('en_core_web_sm')\\ndoc = nlp(paper)\\nquotes = [\\n    Quote(text=sent.text, source_id=paper.id, start=sent.start_char, end=sent.end_char)\\n    for sent in doc.sents\\n]\\n\\nstore(*quotes, collection='quotes')\\n```\\n\\nThis stores the quotes in a `promptx` collection. Let's query that to see what quotes we have.\\n\\n```python\\nfrom promptx import query\\n\\nquotes = query('how does the paper use ensembles?', collection='quotes', limit=10)\\n```\\n![quote-query](./quote-query.png)\\n\\nThe response from `query` is a `Collection`, which extends `pd.Dataframe` - allowing you to use standard pandas filtering and aggregation methods.\\n\\n```python\\n# remove any quotes that are too short\\nquotes = quotes[quotes['text'].str.len() > 25]\\n```\\n\\n## Generating thoughts\\n\\nNext, let's consider what we do when we read something like a paper. We don't just remember quotes, we also have thoughts and ideas. We can do the same thing with **promptx** by defining a `Thought` entity and crafting prompts to elicit interesting thoughts.\\n\\n```python\\nfrom enum import Enum\\nfrom promptx import Entity\\n\\nclass ThoughtType(Enum):\\n    question = 'question'\\n    idea = 'idea'\\n    comment = 'comment'\\n    critique = 'critique'\\n    fact = 'fact'\\n\\nclass Thought(Entity):\\n    text: str\\n    thought_type: ThoughtType\\n    source_id: str\\n    start: int\\n    end: int\\n```\\n\\nAbove, we define a `Thought` entity, but this time we're using an `enum` which will restrict the allowed values for `thought_type`.\\n\\nNow, we can use this to generate thoughts while we're reading the paper. By setting `output=[Thought]` we're telling **promptx** to return a list of `Thought` objects instead of the default string output.\\n\\n```python\\nfrom promptx import prompt\\n\\ndef read(doc: list[str], bs=5, limit=None):\\n    thoughts = []\\n    for chunk in batch(doc, bs=bs, limit=limit):\\n        output = prompt(\\n            '''\\n            Given a passage from a document, generate a list of thoughts \\n            about the passage. Don't repeat yourself!\\n            ''',\\n            input=dict(\\n                passage=chunk,\\n            ),\\n            output=[Thought],\\n        ).objects\\n\\n        thoughts += output\\n    return thoughts\\n```\\n:::info\\nThe output from `prompt` is a `Collection` just like `query`. Above, we convert it to a list of `Entity` objects using the `objects` property.\\n:::\\n\\nTo avoid reading the document again, we can use the stored quotes.\\n\\n```python\\nquotes = query(collection='quotes')\\nread([q.text for q in quotes.objects])\\n```\\n\\n```\\n'Data Augmentation for In-Context Learning via Self-Paraphrase',\\n'ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios.',\\n'DAIL leverages the intuition that large language models are more familiar with the content generated by themselves.',\\n'It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions.',\\n'Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario.',\\n'Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible.',\\n'We believe our work will stimulate further research on ICL in low-resource settings.',\\n'Recently, the rapid development of large language models (LLMs) and their striking skill and knowledge have sparked significant interest in In-Context Learning (ICL).',\\n'Under ICL, there is no parameter adjustment to LLMs.',\\n'The model is given an instruction, task description, in-context samples, and a test case.',\\n'In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks.',\\n'ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios.',\\n'DataAugmentation for In-Context Learning (DAIL) is proposed to overcome this limitation.',\\n'DAIL leverages the intuition that large language models are more familiar with the content generated by themselves.',\\n'It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions.',\\n'Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario.',\\n'Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible.',\\n'We believe our work will stimulate further research on ICL in low-resource settings.',\\n'Recently, the rapid development of large language models (LLMs) (Devlin et al., 2018; Radford et al., 2019) and their striking skill and knowledge have sparked significant interest in In-Context Learning (ICL) (Brown et al., 2020).',\\n'Different from other training-based paradigms, under ICL, there is no parameter adjustment to LLMs.',\\n'In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks.',\\n'ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios.',\\n'Data Augmentation for In-Context Learning (DAIL) is proposed to overcome the limitation of high-quality annotated demonstrations not being available in real-world scenarios.',\\n'DAIL leverages the intuition that large language models are more familiar with the content generated by themselves.',\\n'It first utilizes the language model to\\\\ngenerate paraphrases of the test sample and\\\\nemploys majority voting to determine the final\\\\nresult based on individual predictions.',\\n'Our ex-\\\\ntensive empirical evaluation shows that DAIL\\\\noutperforms the standard ICL method and other\\\\nensemble-based methods in the low-resource\\\\nscenario.',\\n'Additionally, we explore the use of\\\\nvoting consistency as a confidence score of the\\\\nmodel when the logits of predictions are inac-\\\\ncessible.',\\n'We believe our work will stimulate\\\\nfurther research on ICL in low-resource set-\\\\ntings.',\\n'Recently, the rapid development of large language\\\\nmodels (LLMs) (Devlin et al., 2018; Radford et al.,\\\\n2019) and their striking skill and knowledge have\\\\nsparked significant interest in In-Context Learning\\\\n(ICL) (Brown et al., 2020).',\\n'Different from other training-based paradigms, under ICL, there is no parameter adjustment to LLMs.'\\n```\\n\\nThis could be taken a lot further. E.g. by using RAG during thought generation, using [step-back prompts](https://arxiv.org/abs/2310.06117), or with user defined instructions to guide the types of thoughts to generate. However, without a solid way to evaluate the usefulness, it's easy to over-engineer so let's see how it performs first.\\n\\n## Question prompting\\n\\n```python\\ndef qa(question, collection):\\n    context = query(question, collection=collection)\\n    output = prompt(\\n        '''\\n        Given a question and a list of quotes, answer the question.\\n        ''',\\n        input=dict(\\n            question=question,\\n            context=[c.text for c in context.objects]\\n        ),\\n    )\\n    return output\\n```\\n\\nLet's try the standard RAG approach first by just using quotes.\\n\\n```python\\nqa('Can you explain the main thesis of the paper?', collection='quotes')\\n```\\n> The main thesis of the paper is that the proposed method, called DAIL, out-performs other ensemble-based methods for classification. The paper also presents experiments that demonstrate the effectiveness and generalization of the proposed method on coarse-grained and fine-grained classification tasks.\\n\\nThis isn't that helpful - it is able to pick out the name (DAIL) and some aspects of the paper, but isn't able to describe the core idea behind the paper.\\n\\nLet's try it with the generated thoughts.\\n\\n```python\\nqa('What is the name of the proposed method?', collection='thoughts')\\n```\\n> The main thesis of the paper is that a new method called DAIL (In-Context Learning) outperforms the standard ICL method and other ensemble-based methods in low-resource scenarios. **DAIL leverages the intuition that large language models are more familiar with the content generated by themselves**. It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. The authors believe that their work will stimulate further research on ICL in low-resource settings.\\n\\nThis is much better. Crucially, it's identified the main idea behind the paper.\\n\\n## Next steps\\n\\nI haven't tested this on any benchmarks or using any objective metrics, but it seems like a promising approach. In a future post I'll explore how to evaluate the effectiveness of the generated thoughts using `promptx.evaluate()`.\"},{\"id\":\"generating-narratives\",\"metadata\":{\"permalink\":\"/posts/generating-narratives\",\"source\":\"@site/blog/2023-11-05-generating-narratives/index.md\",\"title\":\"Generating Narratives: Writing Coherent Stories with LLMs\",\"description\":\"Cover\",\"date\":\"2023-11-05T00:00:00.000Z\",\"formattedDate\":\"November 5, 2023\",\"tags\":[],\"readingTime\":11.235,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"generating-narratives\",\"title\":\"Generating Narratives: Writing Coherent Stories with LLMs\"},\"unlisted\":false,\"prevItem\":{\"title\":\"Teaching an LLM How to Read\",\"permalink\":\"/posts/teaching-an-llm-how-to-read\"}},\"content\":\"![Cover](./cover.png)\\n\\nModern language models are great at producing, and understanding, short passages of text. But if you've ever tried to generate longer, more complicated content, it can quickly become a confusing mess as the model loses track of what it should focus on.\\n\\nIn this post we'll explore a technique published by [DeepMind](https://deepmind.google/) for creating screenplays using LLMs. Although this is focused on creative writing, I think the techniques could be employed more widely with some modifications.\\n\\nI've created an accompanying [notebook](https://github.com/layterz/promptx/tree/main/examples/dramatron) with all the code below if you want to try it out yourself.\\n\\n\x3c!--truncate--\x3e\\n\\n## Dramatron\\n\\nDeepMind published [Co-Writing Screenplays and Theatre Scripts with Language Models: An Evaluation by Industry Professionals](https://arxiv.org/abs/2209.14958) in late 2022. The paper describes Dramatron \u2013 an algorithm for procedurally generating a coherent story by starting with a logline and sequentially generating a title, characters, an outline, location descriptions, and finally a script. The output of each step being passed to the next.\\n\\n![Figure 1](./dramatron.png)\\n\\nWe'll implement a version of Dramatron using [promptx](https://github.com/layterz/promptx), a framework for working with generative models and embedding vector stores.\\n\\n### Setup\\nFirst we need to install and setup promptx\\n\\n```bash\\npip install pxx\\n```\\n\\nAnd now we need to configure the LLM we want to use. There are a few ways to do this, but for simplicity lets use environment variables.\\n\\n```bash\\nexport PXX_DEFAULT_LLM=chatgpt\\nexport PXX_OPENAI_API_KEY=...\\nexport PXX_OPENAI_ORG_ID=...\\n```\\n\\nLet's test if it's working\\n\\n```python\\nfrom promptx import prompt\\n\\noutput = prompt('Where is the capital of France?')\\nassert 'Paris' in output\\n```\\n\\nIn the simplest case prompt accepts a single instructions argument and returns a string as output, but we'll explore some more advanced uses below.\\n\\n### Step 1: title generation\\n\\nIn screenwriting, a logline is a short 1\u20132 sentence summary of a story that captures the premise and peaks a readers interest. Here's an example\\n\\n> \\\"The patriarch of a powerful Italian mafia family passes the torch to his reluctant son whose moral dilemma and the ensuing power struggles lead to a blood-soaked path of loyalty, honor, and vengeance.\\\"\\n\\nDramatron expects a user defined logline as input and uses it to generate a title and character profiles. Let's start with the title.\\n\\n```python\\nfrom promptx import prompt\\n\\ndef write_title(logline: str) -> str:\\n    return prompt(\\n        '''\\n        Suggest a alternative, original and descriptive \\n        title for a known story.\\n        ''',\\n        dict(logline=logline),\\n    )\\n\\nlogline = '''\\nA computer hacker learns from mysterious rebels about the true \\nnature of his reality and his role in the war against its controllers.\\n'''\\n\\ntitle = write_title(logline)\\n>>> \\\"The Cipher's Awakening: Unveiling the Matrix\\\"\\n```\\n\\nWe're calling prompt with two arguments this time. In addition to the instructions we're also passing an input that the instructions act upon.\\n\\n:::info\\n\\n*Note: I would probably rewrite this prompt to something like:*\\n\\n> Given a logline, suggest an alternative, original and descriptive title for a known story.\\n\\n*That sets the expectation about the input and produces better results using `gpt-3.5-turbo`. However, I want to stick to the paper as much as possible so the alternative I landed on was to use a dict for the input so the key acts as an implicit label.*\\n\\n:::\\n\\nThe generated titles aren't that good\u200a-\u200athey're too long and most have cringe taglines.\\n\\n```\\n\\\"The Cipher's Awakening: Unveiling the Matrix\\\"\\n\\\"The Digital Awakening: Uncovering the Truth in a War for Control\\\"\\n\\\"The Cyber Awakening: Unveiling the Illusionary Enigma\\\"\\n\\\"The Binary Revolution: Unveiling the Matrix of Power\\\"\\n\\\"The Cyber Awakening: Unveiling the Digital Truth\\\"'\\n```\\n\\nThe paper uses few shot examples to guide the model output so let's add those and try again. `prompt` optionally accepts an example= keyword argument that should be a list of tuples representing the expected input/output pattern.\\n\\n```python\\nfrom promptx import prompt\\n\\ndef write_title(logline: str) -> str:\\n    return prompt(\\n        'Suggest a alternative, original and descriptive title for a known story.',\\n        logline,\\n        examples = [\\n            (\\n                star_wars.logline,\\n                \\\"The Death Star's Menace\\\"\\n            ),\\n            (\\n                \\\"Residents of San Fernando Valley are under attack by flying saucers from outer space. The aliens are extraterrestrials who seek to stop humanity from creating a doomsday weapon that could destroy the universe and unleash the living dead to stalk humans who wander into the cemetery looking for evidence of the UFOs. The hero Jeff, an airline pilot, will face the aliens.\\\",\\n                \\\"The Day The Earth Was Saved By Outer Space.\\\"\\n            )\\n        ]\\n    )\\n\\nlogline = '''\\nA computer hacker learns from mysterious rebels about the true \\nnature of his reality and his role in the war against its controllers.\\n'''\\n\\ntitle = write_title(logline)\\n>>> The Matrix Unleashed\\n```\\n\\nRunning this 5 times we get the following\\n\\n```\\n\\\"The Matrix Unleashed\\\"\\n\\\"The Matrix: Unraveling the Code\\\"\\n\\\"The Matrix: Unplugged\\\"\\n\\\"The Matrix: Unplugged\\\" # it liked this one\\n\\\"The Matrix: Unveiling the Digital World\\\"\\n```\\n\\nThese still aren't that good, but it's an improvement. I'm not sure why all the taglines begin with `\\\"Un\\\"`.\\n\\nAgain, I think there are some changes that could be made to the prompts and examples, but let's stick to the paper for now and move on.\\n\\n### Step 2: character profiles\\n\\nThis is more challenging because we need to generate a complex object and not a simple string. promptx lets you define expected output as a [Pydantic](https://docs.pydantic.dev/latest/) object so let's use that to define a character generation prompt.\\n\\nWe can pass an output= keyword argument to prompt, which should be a basic type (e.g. `int`\xa0, `str`\xa0, etc.) or a `BaseModel` subclass. If provided, it will be used to add formatting instructions to the prompt and validate the response.\\n\\n```python\\nfrom pydantic import BaseModel\\n\\nclass Character(BaseModel):\\n    name: str\\n    description: str\\n\\ndef create_characters(logline: str, n=5) -> List[Character]:\\n    return prompt(\\n        f'Create {n} characters for a story.',\\n        input=logline,\\n        output=[Character],\\n        examples=[\\n            (\\n                star_wars.logline,\\n                star_wars.characters,\\n            ),\\n        ],\\n    ).objects\\n\\ncharacters = create_characters(logline, n=3)\\n```\\n\\n*Note: because prompt is returning a list the response will be a `Collection` object. To parse this into a list of Pydantic model instances we use the `objects` property.*\\n\\nHere's some example output from the character generation prompt.\\n\\n```python\\n[  \\n  Character(\\n    type='character',\\n    name='Neo',\\n    description='Neo is the protagonist. A computer hacker who discovers that the world he knows is a simulated reality created by machines. He joins the rebels to fight against the controllers.'\\n  ),\\n  Character(\\n    type='character',\\n    name='Morpheus',\\n    description='Morpheus is the leader of the rebels. He guides Neo in understanding the true nature of reality and his role in the war against the controllers.'\\n  ),\\n  Character(\\n    type='character',\\n    name='Trinity',\\n    description=\\\"Trinity is a skilled hacker and member of the rebellion. She becomes Neo's love interest and fights alongside him against the controllers.\\\"\\n  )\\n]\\n```\\n\\n### Step 3: writing the\xa0outline\\n\\nThe next step is to generate the plot outline as a series of scene \\\"beats\\\", which consist of a location, scene description, and plot element. The plot element describes narrative stage: e.g. exposition or \\\"a call to adventure\\\". The specific terminology depends on the narrative framework being used.\\n\\nThe paper describes two different narrative frameworks: Freytag's pyramid and the hero's journey. Both are ways to break down the narrative progress of stories in a systematic way. We'll use the hero's journey\u200a-\u200amostly because the Star Wars examples in the paper use that format.\\n\\n![Hero's Journey](./heros-journey.png)\\n\\nWe need to define a `SceneBeat` object. We can use an `enum` to constrain the plot element field to the expected strings.\\n\\n```python\\nfrom enum import Enum\\n\\nclass PlotElement(str, Enum):\\n    call_to_adventure = \\\"Call to Adventure\\\"\\n    refusal_of_call = \\\"Refusal of the Call\\\"\\n    supernatural_aid = \\\"Supernatural Aid\\\"\\n    crossing_the_threshold = \\\"Crossing the Threshold\\\"\\n    belly_of_the_whale = \\\"Belly of the Whale\\\"\\n    road_of_trials = \\\"Road of Trials\\\"\\n    meeting_with_the_goddess = \\\"Meeting with the Goddess\\\"\\n\\nclass SceneBeat(BaseModel):\\n    location: str\\n    plot_element: str\\n    description: str\\n\\ndef write_outline(logline: str, characters: list[Character], n=10) -> List[SceneBeat]:\\n    return prompt(\\n        f'''\\n        Write a sequence of {n} scene beats for a story a hero's journey structure.\\n        ''',\\n        input=dict(logline=logline, characters=characters),\\n        output=[SceneBeat],\\n        examples=[\\n            (\\n                dict(\\n                    logline=star_wars.logline,\\n                    characters=star_wars.characters,\\n                ),\\n                star_wars.outline,\\n            ),\\n        ],\\n    ).objects\\n\\noutline = write_outline(story.logline, story.characters)\\n```\\n\\nThese are the first three scenes beats generated.\\n\\n```python\\n[\\n    Scenebeat(\\n        type='scenebeat',\\n        location=\\\"Neo's apartment\\\",\\n        plot_element='The Ordinary World',\\n        description='Neo is living a mundane life as a computer hacker, unaware of the true nature of his reality.'\\n    ),\\n    Scenebeat(\\n        type='scenebeat',\\n        location='Unknown location',\\n        plot_element='Call to Adventure',\\n        description='Neo is contacted by the mysterious rebels led by Morpheus. They inform him that the world he knows is a simulation created by machines, and that he is the chosen one who can lead the rebellion against the controllers.\\\\n            '\\n    ),\\n    Scenebeat(\\n        type='scenebeat',\\n        location='Abandoned building',\\n        plot_element='Refusal of the Call',\\n        description=\\\"Neo initially doubts his abilities and refuses to believe the truth. However, after encountering Agent Smith and narrowly escaping, he starts to consider Morpheus' offer.\\\\n            \\\"\\n    ),\\n    ...\\n]\\n```\\n\\n### Step 4: extracting locations\\n\\nNow we have an outline we can extract a location from each scene beat and generate a description for it. We keep track of known locations to avoid generating the same thing multiple times.\\n\\n```python\\nclass Location(BaseModel):\\n    name: str\\n    description: str\\n\\ndef extract_locations(outline) -> List[Location]:\\n    locations = []\\n    for beat in outline:\\n        if beat.location in [l.name for l in locations]:\\n            continue\\n        response = prompt(\\n            '''\\n            Generate a location based on location name. \\n            ''',\\n            input=dict(name=beat.location),\\n            output=Location,\\n        )\\n    return locations\\n\\nlocations = extract_locations(outline)\\n```\\n\\nNote: I omitted the examples for brevity. See the notebook for the full prompt.\\n\\nHere are the first three locations generated.\\n\\n```python\\n[\\n    Location(\\n        type='location',\\n        name=\\\"Neo's apartment\\\",\\n        description=\\\"Neo's apartment is small and cluttered, with stacks of computer \\\\n                        equipment and electronic gadgets scattered everywhere. The walls \\\\n                        are covered in posters and graffiti, giving the place a rebellious \\\\n                        and underground feel. The room is illuminated by the glow of computer \\\\n                        screens and neon lights, creating a moody and mysterious atmosphere. \\\\n                        \\\"\\n    ),\\n    Location(\\n        type='location',\\n        name='Unknown location',\\n        description='The location is not known. Please provide a valid known location.\\\\n                        '\\n    ),\\n    Location(\\n        type='location',\\n        name='Abandoned building',\\n        description='The abandoned building is a dilapidated structure, once \\\\n                        bustling with life but now left to decay. Broken windows, \\\\n                        crumbling walls, and graffiti cover the exterior. Inside, \\\\n                        debris litters the floor, and the air is thick with dust. \\\\n                        Shadows dance in the dim light, giving the space an eerie \\\\n                        atmosphere.\\\\n                        '\\n    ),\\n    ...\\n]\\n```\\n\\nIt failed to produce real output for the second location because the name \\\"Unknown location\\\" confused the model. Adding an example to cover the expected behavior might help.\\n\\nHow to handle invalid content and how to detect valid, but \\\"incorrect\\\" content is a larger issue, which I may cover in another post.\\n\\n### Step 5: putting it all\xa0together\\n\\nNow we have generated all the components, we can combine them to write a script for each scene. Before we define that, let's create a `Story` object to hold the data we've generated.\\n\\n```python\\nclass Story(BaseModel):\\n    logline: str\\n    title: str = None\\n    outline: List[SceneBeat] = None\\n    characters: List[Character] = None\\n    locations: List[Location] = None\\n\\n    def __init__(self, logline, **kwargs):\\n        super().__init__(logline=logline, **kwargs)\\n```\\n\\nDramatron generates the script by iterating over each scene in the outline and passing the current beat description and plot element, the previous beat (if any), and the relevant details generated for each scene.\\n\\n```python\\ndef write_scene(story: Story, beat: SceneBeat, previous_beat: SceneBeat = None) -> str:\\n    try:\\n        location = next(filter(lambda x: x.name.lower() == beat.location.lower(), story.locations))\\n    except StopIteration:\\n        print(f'No location found for {beat.location}')\\n        return None\\n    \\n    return prompt(\\n        f'''\\n        Write a scene for a story based on the scene beat and location.\\n        ''',\\n        input=dict(\\n            plot_element=beat.plot_element,\\n            beat_description=beat.description,\\n            location=location.name,\\n            characters=story.characters,\\n            logline=story.logline,\\n            title=story.title,\\n            previous_beat=previous_beat,\\n        ),\\n    )\\n\\ndef write_script(story: Story, k: int = None) -> list[str]:\\n    previous_beat = None\\n    for beat in story.outline[:k or len(story.outline)]:\\n        scene = write_scene(story, beat, previous_beat)\\n        previous_beat = beat\\n\\nscript = write_script(story)\\n```\\n\\nHere's the first page or so.\\n\\n>**INT. NEO'S APARTMENT\u200a-\u200aLIVING ROOM\u200a-\u200aNIGHT**\\n>\\n>*The room is dimly lit, with the glow of computer screens illuminating Neo's face. He sits hunched over his desk, surrounded by multiple monitors displaying complex codes and algorithms.*\\n>\\n>*Neo, a young man in his late 20s with disheveled hair and tired eyes, furiously types away on his keyboard. He is a computer hacker, known in the online world as a genius. But in the real world, he leads a mundane and unfulfilled life.*\\n>\\n>*The sound of a knock on the door interrupts Neo's concentration. He reluctantly looks up and walks towards the door, curiosity piqued.*\\n>\\n>**NEO (sighs): Who could that be at this hour?**\\n>\\n>*Neo cautiously opens the door, revealing MORPHEUS, a tall and enigmatic man in his 40s. He wears a long, dark coat and exudes an air of mystery and authority.*\\n>\\n>**MORPHEUS (smiling): Hello, Neo. We meet at last.**\\n>\\n>*Confused, Neo looks at Morpheus, unsure of how this stranger knows his name.*\\n>\\n>**NEO (nervously): Who are you? How do you know my name?**\\n>\\n>*Morpheus steps inside the apartment, and Neo instinctively backs away, wary of this unexpected visitor.*\\n>\\n>**MORPHEUS: I know a lot about you, Neo. More than you think.**\\n>\\n>*Neo's eyes widen in disbelief, his curiosity growing.*\\n>\\n>**NEO (stammering): How\u2026 How is that possible?**\\n>\\n>*Morpheus smirks and reaches inside his coat, revealing a small, sleek device. He holds it out to Neo.*\\n>\\n>**MORPHEUS (whispering): Take this, Neo. It will show you the truth.**\\n>\\n>*Neo hesitates for a moment before cautiously accepting\xa0\u2026*\\n>\\n>[CONTINUED]\\n\\n## Conclusion\\n\\nDoes Dramatron produce great writing? No. But, hopefully this has demonstrated some techniques for controlling LLMs and combining the outputs of multiple prompts in coherent ways.\\n\\nIn a future post I'll explore some ideas to extend Dramatron using embeddings to combine generated outputs in more meaningful ways.\"}]}")}}]);